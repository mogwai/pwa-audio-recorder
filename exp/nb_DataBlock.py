
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/DataBlock.ipynb


#Internal dependencies
from pathlib import Path as PosixPath
from exp.nb_AudioCommon import *

import os
#External dependencies
from fastai.utils import *

from fastai.vision import *
from IPython.display import Audio
import torchaudio
from torchaudio import transforms

class AudioItem(ItemBase):
    def __init__(self, ad:AudioData, **kwargs):
        self.ad = ad
        self.kwargs = kwargs

    def __str__(self):
        if isinstance(self.ad, AudioData): return f'{self.__class__.__name__}: {self.duration}sec ({self.ad.sig.shape[0]} @ {self.ad.sr}hz).'
        else: return f'{type(self.data)}: {self.data.shape}'

    def __len__(self): return self.data.shape[0]
    def _repr_html_(self): return f'{self.__str__()}<br />{self.ipy_audio._repr_html_()}'

    def show(self, title:Optional[str]=None, **kwargs):
        "Show sound on `ax` with `title`, using `cmap` if single-channel, overlaid with optional `y`"
        self.hear(title=title)
        if self.ad.spectro is not None:
            display(Image(self.ad.spectro))

    def hear(self, title=None):
        if title is not None: print(title)
        display(self.ipy_audio)

    def apply_tfms(self, tfms):
        for tfm in tfms:
            self.data = tfm(self.data)
        return self

    @property
    def shape(self):
        return self.data.shape

    @property
    def ipy_audio(self):
        return Audio(data=self.ad.sig, rate=self.ad.sr)

    @property
    def duration(self): return len(self.ad.sig)/self.ad.sr

    @property
    def data(self): return self.ad.spectro if self.ad.use_spectro else self.ad.sig
    @data.setter
    def data(self, x):
        if self.ad.use_spectro:
            self.ad.spectro = x
        else:
            self.ad.sig = x


class AudioDataBunch(DataBunch):
    def hear_ex(self, rows:int=3, ds_type:DatasetType=DatasetType.Valid, **kwargs):
        batch = self.dl(ds_type).dataset[:rows]
        self.train_ds.hear_xys(batch.x, batch.y, **kwargs)

class AudioList(ItemList):
    _bunch = AudioDataBunch

    # TODO: __REPR__
    def __init__(self, items, *args, use_spectro=True, cache_spectro=True, to_db_scale=True, force_cache=False, n_fft=1024,
                ws=None, hop=72, f_min=0.0, f_max=200, pad=0, n_mels=224, max_to_pad=160000, **kwargs):
        super().__init__(items, *args, **kwargs)
        self.tfm_args = {
            'use_spectro': use_spectro,
            'cache_spectro': cache_spectro,
            'to_db_scale': to_db_scale,
            'force_cache': force_cache,
            'n_fft':n_fft,
            'ws':ws,
            'hop':hop,
            'f_min':f_min,
            'f_max':f_max,
            'pad':pad,
            'n_mels':n_mels,
            'max_to_pad':max_to_pad,
        }

    def open(self, item):
        if isinstance(item, (PosixPath, Path, str)):
            return AudioItem(AudioData.load(self.path/item, **self.tfm_args))
        if isinstance(item, (tuple, np.ndarray)): #data,sr
            return AudioItem(AudioData(item[0],item[1]))
        print('Format not supported!', file=sys.stderr)
        raise

    def get(self, i):
        item = self.items[i]
        return self.open(item)

    def reconstruct(self, t:Tensor): return Image(t.transpose(1,2)) #FIXME!! No Image here

    def hear_xys(self, xs, ys, **kwargs):
        for x, y in zip(xs, ys): x.hear(title=y, **kwargs)

    # TODO: example with from_folder
    @classmethod
    def from_folder(cls, path:PathOrStr='.', extensions:Collection[str]=None, recurse:bool=True, **kwargs)->ItemList:
        if not extensions: extensions = AUDIO_EXTENSIONS
        return cls(get_files(path, extensions, recurse), path, **kwargs)